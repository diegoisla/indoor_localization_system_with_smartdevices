{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = os.pardir\n",
    "\n",
    "RAW_DATA_PATH = ROOT_PATH + '\\\\data\\\\raw\\\\'\n",
    "\n",
    "PROC_DATA_PATH = ROOT_PATH + '\\\\data\\\\proc\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>place_id</th>\n",
       "      <th>WAP001</th>\n",
       "      <th>WAP002</th>\n",
       "      <th>WAP003</th>\n",
       "      <th>WAP004</th>\n",
       "      <th>WAP005</th>\n",
       "      <th>WAP006</th>\n",
       "      <th>WAP007</th>\n",
       "      <th>WAP008</th>\n",
       "      <th>WAP009</th>\n",
       "      <th>...</th>\n",
       "      <th>WAP123</th>\n",
       "      <th>WAP124</th>\n",
       "      <th>WAP125</th>\n",
       "      <th>WAP126</th>\n",
       "      <th>WAP127</th>\n",
       "      <th>MagneticFieldX</th>\n",
       "      <th>MagneticFieldY</th>\n",
       "      <th>MagneticFieldZ</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-15.4</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-33.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-15.4</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-33.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-15.4</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-33.3</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-15.4</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-33.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-33.7</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11444</th>\n",
       "      <td>324</td>\n",
       "      <td>-69</td>\n",
       "      <td>-69</td>\n",
       "      <td>-50</td>\n",
       "      <td>-49</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-75</td>\n",
       "      <td>-76</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-100</td>\n",
       "      <td>-78</td>\n",
       "      <td>-77</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>14.1</td>\n",
       "      <td>-37.2</td>\n",
       "      <td>-19.6</td>\n",
       "      <td>32.4</td>\n",
       "      <td>34.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11446</th>\n",
       "      <td>324</td>\n",
       "      <td>-69</td>\n",
       "      <td>-69</td>\n",
       "      <td>-50</td>\n",
       "      <td>-49</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-75</td>\n",
       "      <td>-76</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-100</td>\n",
       "      <td>-78</td>\n",
       "      <td>-77</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>14.1</td>\n",
       "      <td>-36.8</td>\n",
       "      <td>-19.6</td>\n",
       "      <td>32.4</td>\n",
       "      <td>34.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11447</th>\n",
       "      <td>324</td>\n",
       "      <td>-69</td>\n",
       "      <td>-69</td>\n",
       "      <td>-50</td>\n",
       "      <td>-49</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-75</td>\n",
       "      <td>-76</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-100</td>\n",
       "      <td>-78</td>\n",
       "      <td>-77</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>14.1</td>\n",
       "      <td>-36.8</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>34.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11448</th>\n",
       "      <td>324</td>\n",
       "      <td>-69</td>\n",
       "      <td>-69</td>\n",
       "      <td>-50</td>\n",
       "      <td>-49</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-75</td>\n",
       "      <td>-76</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-100</td>\n",
       "      <td>-78</td>\n",
       "      <td>-77</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>14.5</td>\n",
       "      <td>-36.8</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>34.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11455</th>\n",
       "      <td>324</td>\n",
       "      <td>-69</td>\n",
       "      <td>-69</td>\n",
       "      <td>-50</td>\n",
       "      <td>-49</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>-75</td>\n",
       "      <td>-76</td>\n",
       "      <td>-100</td>\n",
       "      <td>...</td>\n",
       "      <td>-100</td>\n",
       "      <td>-78</td>\n",
       "      <td>-77</td>\n",
       "      <td>-100</td>\n",
       "      <td>-100</td>\n",
       "      <td>14.5</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>32.4</td>\n",
       "      <td>34.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5070 rows × 133 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       place_id  WAP001  WAP002  WAP003  WAP004  WAP005  WAP006  WAP007  \\\n",
       "3             2    -100    -100    -100    -100    -100    -100    -100   \n",
       "6             2    -100    -100    -100    -100    -100    -100    -100   \n",
       "7             2    -100    -100    -100    -100    -100    -100    -100   \n",
       "9             2    -100    -100    -100    -100    -100    -100    -100   \n",
       "14            2    -100    -100    -100    -100    -100    -100    -100   \n",
       "...         ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "11444       324     -69     -69     -50     -49    -100    -100     -75   \n",
       "11446       324     -69     -69     -50     -49    -100    -100     -75   \n",
       "11447       324     -69     -69     -50     -49    -100    -100     -75   \n",
       "11448       324     -69     -69     -50     -49    -100    -100     -75   \n",
       "11455       324     -69     -69     -50     -49    -100    -100     -75   \n",
       "\n",
       "       WAP008  WAP009  ...  WAP123  WAP124  WAP125  WAP126  WAP127  \\\n",
       "3        -100    -100  ...    -100    -100    -100    -100    -100   \n",
       "6        -100    -100  ...    -100    -100    -100    -100    -100   \n",
       "7        -100    -100  ...    -100    -100    -100    -100    -100   \n",
       "9        -100    -100  ...    -100    -100    -100    -100    -100   \n",
       "14       -100    -100  ...    -100    -100    -100    -100    -100   \n",
       "...       ...     ...  ...     ...     ...     ...     ...     ...   \n",
       "11444     -76    -100  ...    -100     -78     -77    -100    -100   \n",
       "11446     -76    -100  ...    -100     -78     -77    -100    -100   \n",
       "11447     -76    -100  ...    -100     -78     -77    -100    -100   \n",
       "11448     -76    -100  ...    -100     -78     -77    -100    -100   \n",
       "11455     -76    -100  ...    -100     -78     -77    -100    -100   \n",
       "\n",
       "       MagneticFieldX  MagneticFieldY  MagneticFieldZ     x     y  \n",
       "3               -15.4            -0.2           -33.3   1.2   0.6  \n",
       "6               -15.4            -0.8           -33.3   1.2   0.6  \n",
       "7               -15.4            -1.2           -33.3   1.2   0.6  \n",
       "9               -15.4            -1.2           -33.7   1.2   0.6  \n",
       "14              -15.0            -1.2           -33.7   1.2   0.6  \n",
       "...               ...             ...             ...   ...   ...  \n",
       "11444            14.1           -37.2           -19.6  32.4  34.2  \n",
       "11446            14.1           -36.8           -19.6  32.4  34.2  \n",
       "11447            14.1           -36.8           -20.0  32.4  34.2  \n",
       "11448            14.5           -36.8           -20.0  32.4  34.2  \n",
       "11455            14.5           -36.4           -20.0  32.4  34.2  \n",
       "\n",
       "[5070 rows x 133 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signals = pd.read_excel(PROC_DATA_PATH + 'measure1_wifi_smartphone.xlsx', index_col=0)\n",
    "signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split labels and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = m1_wifi_df.iloc[:,:-2]\n",
    "# y = m1_wifi_df.iloc[:,-2:]\n",
    "\n",
    "X = signals.iloc[:,1:-2]\n",
    "y = signals.iloc[:,-2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std = StandardScaler().fit(X)\n",
    "X = pd.DataFrame(std.transform(X), columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle           \n",
    "\n",
    "X, y = shuffle(X, y, random_state=44)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=44)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of Linear Regression: 2.1990369634738767\n",
      "RMSE of Random Forest: 0.03513835713308024\n",
      "RMSE of Gradient Boosting wrapped: 0.9664799409358944\n",
      "RMSE of Extra Tree: 0.023100756922971685\n",
      "RMSE of Decision Tree: 0.061055799486105906\n",
      "RMSE of XGB wrapped: 0.059858334533271496\n",
      "RMSE of Ridge: 2.192476334080066\n",
      "RMSE of LASSO: 3.821376515985804\n",
      "RMSE of LGBM wrapped: 0.22694311876837006\n",
      "RMSE of CatBoost: 0.22339659208895896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression,Ridge,Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,AdaBoostRegressor,StackingRegressor,ExtraTreesRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "#Running multiple models with Pipeline\n",
    "models = []  # Empty list to store all the models\n",
    "\n",
    "# Appending pipelines for each model into the list\n",
    "models.append(\n",
    "    (\n",
    "        \"Linear Regression\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"lin_reg\", LinearRegression(n_jobs=-1)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"Random Forest\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"random_forest\", RandomForestRegressor(random_state=42,n_jobs=-1)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"Gradient Boosting wrapped\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"gradient_boosting\", MultiOutputRegressor(\n",
    "                                                            GradientBoostingRegressor(random_state=42))\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"Extra Tree\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"ExTree\", ExtraTreesRegressor()),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "\n",
    "models.append(\n",
    "    (\n",
    "        \"Decision Tree\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"decision_tree\", DecisionTreeRegressor(random_state=42)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"XGB wrapped\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"xgboost\", MultiOutputRegressor(\n",
    "                            XGBRegressor(random_state=42,eval_metric='logloss',n_jobs=-1))\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"Ridge\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"Ridge\", Ridge(random_state=42)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"LASSO\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"lasso\", Lasso(random_state=42)),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"LGBM wrapped\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"lgbm\", MultiOutputRegressor(\n",
    "                                                LGBMRegressor(random_state=42))\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"CatBoost\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"cat\", CatBoostRegressor(verbose=0,\n",
    "                                          loss_function='MultiRMSE', \n",
    "                                          eval_metric='MultiRMSE')),\n",
    "            ]\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "names = []  # Empty list to store name of the models\n",
    "# loop through all models to get the RMSE\n",
    "results=[]\n",
    "for name, model in models:\n",
    "    names.append(names)\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred=model.predict(X_test)\n",
    "    rmse=np.sqrt(mean_squared_error(y_test,y_pred))\n",
    "    print(\"RMSE of {}: {}\".format(name, rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of models to evaluate: 12\n",
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n",
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n",
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\src\\theBridge\\course-env\\lib\\site-packages\\sklearn\\svm\\_base.py:1201: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "c:\\src\\theBridge\\course-env\\lib\\site-packages\\sklearn\\svm\\_base.py:1201: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n",
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n",
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n",
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n",
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n",
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n",
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n",
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n",
      "Fitting 15 folds for each of 1 candidates, totalling 15 fits\n"
     ]
    }
   ],
   "source": [
    "#Running multiple models with Pipeline\n",
    "models = []  # Empty list to store all the models\n",
    "\n",
    "# Appending pipelines for each model into the list\n",
    "models.append(\n",
    "    (\n",
    "        \"Linear Regression\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"lin_reg\", LinearRegression(n_jobs=-1)),\n",
    "            ]\n",
    "        ),\n",
    "        {}\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"KNeighbors\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"knn\", KNeighborsRegressor(n_jobs=-1)),\n",
    "            ]\n",
    "        ),\n",
    "        {}\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"Gradient Boosting wrapped\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"gradient_boosting\", MultiOutputRegressor(\n",
    "                                                            LinearSVR())\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        {}\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"Extra Tree\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"ExTree\", ExtraTreesRegressor()),\n",
    "            ]\n",
    "        ),\n",
    "        {}\n",
    "    )\n",
    ")\n",
    "\n",
    "models.append(\n",
    "    (\n",
    "        \"Decision Tree\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"decision_tree\", DecisionTreeRegressor(random_state=42)),\n",
    "            ]\n",
    "        ),\n",
    "        {}\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"Random Forest\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"random_forest\", RandomForestRegressor(random_state=42,n_jobs=-1)),\n",
    "            ]\n",
    "        ),\n",
    "        {}\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"Gradient Boosting wrapped\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"gradient_boosting\", MultiOutputRegressor(\n",
    "                                                            GradientBoostingRegressor(random_state=42))\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        {}\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"XGB wrapped\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"xgboost\", MultiOutputRegressor(\n",
    "                            XGBRegressor(random_state=42,eval_metric='logloss',n_jobs=-1))\n",
    "                )\n",
    "            ]\n",
    "        ),\n",
    "        {}\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"Ridge\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"Ridge\", Ridge(random_state=42)),\n",
    "            ]\n",
    "        ),\n",
    "        {}\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"LASSO\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"lasso\", Lasso(random_state=42)),\n",
    "            ]\n",
    "        ),\n",
    "        {}\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"LGBM wrapped\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"lgbm\", MultiOutputRegressor(\n",
    "                                                LGBMRegressor(random_state=42))\n",
    "                ),\n",
    "            ]\n",
    "        ),\n",
    "        {}\n",
    "    )\n",
    ")\n",
    "models.append(\n",
    "    (\n",
    "        \"CatBoost\",\n",
    "        Pipeline(\n",
    "            steps=[\n",
    "                (\"cat\", CatBoostRegressor(verbose=0,\n",
    "                                          loss_function='MultiRMSE', \n",
    "                                          eval_metric='MultiRMSE')),\n",
    "            ]\n",
    "        ),\n",
    "        {}\n",
    "    )\n",
    ")\n",
    "\n",
    "print('Number of models to evaluate:', len(models))\n",
    "\n",
    "# Declaro en un diccionario los pipelines e hiperparametros\n",
    "models_gridsearch = {}\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=3, random_state=44)\n",
    "\n",
    "for m in models:\n",
    "    models_gridsearch[m[0]] = GridSearchCV(m[1],\n",
    "                                          m[2],\n",
    "                                          cv=cv,\n",
    "                                          scoring=\"neg_mean_squared_error\",\n",
    "                                          verbose=1,\n",
    "                                          n_jobs=-1)\n",
    "    \n",
    "    models_gridsearch[m[0]].fit(X_train, y_train)      \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grid</th>\n",
       "      <th>Best score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>-0.009849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>-0.038432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighbors</td>\n",
       "      <td>-0.039871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>-0.047538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>-0.072469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGB wrapped</td>\n",
       "      <td>-0.080796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LGBM wrapped</td>\n",
       "      <td>-0.094547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting wrapped</td>\n",
       "      <td>-1.328357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>-5.731186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>-5.741624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LASSO</td>\n",
       "      <td>-16.103125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Grid  Best score\n",
       "3                  Extra Tree   -0.009849\n",
       "4               Decision Tree   -0.038432\n",
       "1                  KNeighbors   -0.039871\n",
       "5               Random Forest   -0.047538\n",
       "10                   CatBoost   -0.072469\n",
       "6                 XGB wrapped   -0.080796\n",
       "9                LGBM wrapped   -0.094547\n",
       "2   Gradient Boosting wrapped   -1.328357\n",
       "7                       Ridge   -5.731186\n",
       "0           Linear Regression   -5.741624\n",
       "8                       LASSO  -16.103125"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_grids = [(i, j.best_score_) for i, j in models_gridsearch.items()]\n",
    "\n",
    "best_grids = pd.DataFrame(best_grids, columns=[\"Grid\", \"Best score\"]).sort_values(by=\"Best score\", ascending=False)\n",
    "best_grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# define the model\n",
    "model_NN = Sequential()\n",
    "model_NN.add(Dense(1000, input_dim=130, kernel_initializer='he_uniform', activation='relu'))\n",
    "model_NN.add(Dense(300, activation='relu'))\n",
    "model_NN.add(Dense(2))\n",
    "model_NN.compile(loss='mse',\n",
    "              optimizer='adam',\n",
    "              metrics = ['mae','mse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "325/325 [==============================] - 3s 8ms/step - loss: 13.3425 - mae: 1.9458 - mse: 13.3425 - val_loss: 4.6693 - val_mae: 1.5607 - val_mse: 4.6693\n",
      "Epoch 2/100\n",
      "325/325 [==============================] - 2s 7ms/step - loss: 3.4934 - mae: 1.0582 - mse: 3.4934 - val_loss: 2.9914 - val_mae: 1.0199 - val_mse: 2.9914\n",
      "Epoch 3/100\n",
      "325/325 [==============================] - 3s 9ms/step - loss: 3.0928 - mae: 1.0136 - mse: 3.0928 - val_loss: 2.1592 - val_mae: 0.8323 - val_mse: 2.1592\n",
      "Epoch 4/100\n",
      "325/325 [==============================] - 2s 7ms/step - loss: 2.5573 - mae: 0.8375 - mse: 2.5573 - val_loss: 1.4060 - val_mae: 0.6007 - val_mse: 1.4060\n",
      "Epoch 5/100\n",
      "325/325 [==============================] - 2s 6ms/step - loss: 2.5839 - mae: 0.8662 - mse: 2.5839 - val_loss: 2.3321 - val_mae: 0.7542 - val_mse: 2.3321\n",
      "Epoch 6/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 2.2120 - mae: 0.7539 - mse: 2.2120 - val_loss: 1.5707 - val_mae: 0.6038 - val_mse: 1.5707\n",
      "Epoch 7/100\n",
      "325/325 [==============================] - 1s 5ms/step - loss: 2.0570 - mae: 0.7663 - mse: 2.0570 - val_loss: 1.6116 - val_mae: 0.7613 - val_mse: 1.6116\n",
      "Epoch 8/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 2.1589 - mae: 0.7825 - mse: 2.1589 - val_loss: 1.4338 - val_mae: 0.5699 - val_mse: 1.4338\n",
      "Epoch 9/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 1.9424 - mae: 0.7037 - mse: 1.9424 - val_loss: 1.8946 - val_mae: 0.8958 - val_mse: 1.8946\n",
      "Epoch 10/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 1.6272 - mae: 0.6351 - mse: 1.6272 - val_loss: 1.2045 - val_mae: 0.6102 - val_mse: 1.2045\n",
      "Epoch 11/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 1.6895 - mae: 0.6799 - mse: 1.6895 - val_loss: 1.3660 - val_mae: 0.4945 - val_mse: 1.3660\n",
      "Epoch 12/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 1.9706 - mae: 0.7286 - mse: 1.9706 - val_loss: 2.8214 - val_mae: 1.0281 - val_mse: 2.8214\n",
      "Epoch 13/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 1.9274 - mae: 0.7284 - mse: 1.9274 - val_loss: 1.1240 - val_mae: 0.5546 - val_mse: 1.1240\n",
      "Epoch 14/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 1.1921 - mae: 0.5259 - mse: 1.1921 - val_loss: 1.4217 - val_mae: 0.6126 - val_mse: 1.4217\n",
      "Epoch 15/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 1.5245 - mae: 0.6116 - mse: 1.5245 - val_loss: 0.9076 - val_mae: 0.5556 - val_mse: 0.9076\n",
      "Epoch 16/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 1.5427 - mae: 0.5984 - mse: 1.5427 - val_loss: 1.0460 - val_mae: 0.6067 - val_mse: 1.0460\n",
      "Epoch 17/100\n",
      "325/325 [==============================] - 1s 5ms/step - loss: 1.4432 - mae: 0.5969 - mse: 1.4432 - val_loss: 0.9513 - val_mae: 0.4919 - val_mse: 0.9513\n",
      "Epoch 18/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 1.3447 - mae: 0.6002 - mse: 1.3447 - val_loss: 0.9818 - val_mae: 0.4172 - val_mse: 0.9818\n",
      "Epoch 19/100\n",
      "325/325 [==============================] - 2s 6ms/step - loss: 1.3090 - mae: 0.5761 - mse: 1.3090 - val_loss: 0.7355 - val_mae: 0.4361 - val_mse: 0.7355\n",
      "Epoch 20/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 1.4188 - mae: 0.6344 - mse: 1.4188 - val_loss: 1.2127 - val_mae: 0.7000 - val_mse: 1.2127\n",
      "Epoch 21/100\n",
      "325/325 [==============================] - 1s 5ms/step - loss: 1.0316 - mae: 0.5120 - mse: 1.0316 - val_loss: 1.1962 - val_mae: 0.4362 - val_mse: 1.1962\n",
      "Epoch 22/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 1.1967 - mae: 0.5539 - mse: 1.1967 - val_loss: 0.6907 - val_mae: 0.3953 - val_mse: 0.6907\n",
      "Epoch 23/100\n",
      "325/325 [==============================] - 2s 7ms/step - loss: 1.1623 - mae: 0.5421 - mse: 1.1623 - val_loss: 1.0960 - val_mae: 0.6481 - val_mse: 1.0960\n",
      "Epoch 24/100\n",
      "325/325 [==============================] - 2s 6ms/step - loss: 1.4157 - mae: 0.6077 - mse: 1.4157 - val_loss: 1.0567 - val_mae: 0.5194 - val_mse: 1.0567\n",
      "Epoch 25/100\n",
      "325/325 [==============================] - 1s 5ms/step - loss: 0.9895 - mae: 0.5136 - mse: 0.9895 - val_loss: 1.5323 - val_mae: 0.4488 - val_mse: 1.5323\n",
      "Epoch 26/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 1.0122 - mae: 0.4854 - mse: 1.0122 - val_loss: 1.5758 - val_mae: 0.5334 - val_mse: 1.5758\n",
      "Epoch 27/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 1.0270 - mae: 0.4788 - mse: 1.0270 - val_loss: 0.9508 - val_mae: 0.5640 - val_mse: 0.9508\n",
      "Epoch 28/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 1.0529 - mae: 0.5282 - mse: 1.0529 - val_loss: 0.7022 - val_mae: 0.4851 - val_mse: 0.7022\n",
      "Epoch 29/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.8445 - mae: 0.4255 - mse: 0.8445 - val_loss: 0.5556 - val_mae: 0.2539 - val_mse: 0.5556\n",
      "Epoch 30/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.9824 - mae: 0.4758 - mse: 0.9824 - val_loss: 1.1449 - val_mae: 0.5452 - val_mse: 1.1449\n",
      "Epoch 31/100\n",
      "325/325 [==============================] - 2s 6ms/step - loss: 0.9164 - mae: 0.4616 - mse: 0.9164 - val_loss: 0.8152 - val_mae: 0.4468 - val_mse: 0.8152\n",
      "Epoch 32/100\n",
      "325/325 [==============================] - 2s 6ms/step - loss: 0.9094 - mae: 0.5092 - mse: 0.9094 - val_loss: 0.6342 - val_mae: 0.3179 - val_mse: 0.6342\n",
      "Epoch 33/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.7663 - mae: 0.4279 - mse: 0.7663 - val_loss: 0.5845 - val_mae: 0.3119 - val_mse: 0.5845\n",
      "Epoch 34/100\n",
      "325/325 [==============================] - 2s 6ms/step - loss: 0.7728 - mae: 0.4222 - mse: 0.7728 - val_loss: 0.5024 - val_mae: 0.3085 - val_mse: 0.5024\n",
      "Epoch 35/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.7117 - mae: 0.3890 - mse: 0.7117 - val_loss: 0.4271 - val_mae: 0.2429 - val_mse: 0.4271\n",
      "Epoch 36/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.7373 - mae: 0.3872 - mse: 0.7373 - val_loss: 1.1504 - val_mae: 0.7132 - val_mse: 1.1504\n",
      "Epoch 37/100\n",
      "325/325 [==============================] - 1s 5ms/step - loss: 1.2199 - mae: 0.6276 - mse: 1.2199 - val_loss: 1.4084 - val_mae: 0.7629 - val_mse: 1.4084\n",
      "Epoch 38/100\n",
      "325/325 [==============================] - 1s 5ms/step - loss: 0.9603 - mae: 0.4787 - mse: 0.9603 - val_loss: 0.7194 - val_mae: 0.2991 - val_mse: 0.7194\n",
      "Epoch 39/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.6520 - mae: 0.3440 - mse: 0.6520 - val_loss: 0.4329 - val_mae: 0.2431 - val_mse: 0.4329\n",
      "Epoch 40/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.6094 - mae: 0.3337 - mse: 0.6094 - val_loss: 0.8838 - val_mae: 0.3936 - val_mse: 0.8838\n",
      "Epoch 41/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.7280 - mae: 0.3861 - mse: 0.7280 - val_loss: 0.6242 - val_mae: 0.2592 - val_mse: 0.6242\n",
      "Epoch 42/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.6344 - mae: 0.3629 - mse: 0.6344 - val_loss: 0.4068 - val_mae: 0.3352 - val_mse: 0.4068\n",
      "Epoch 43/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.6675 - mae: 0.3752 - mse: 0.6675 - val_loss: 0.6943 - val_mae: 0.4682 - val_mse: 0.6943\n",
      "Epoch 44/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 1.0541 - mae: 0.4747 - mse: 1.0541 - val_loss: 0.6566 - val_mae: 0.3603 - val_mse: 0.6566\n",
      "Epoch 45/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.8222 - mae: 0.4163 - mse: 0.8222 - val_loss: 0.3628 - val_mae: 0.2596 - val_mse: 0.3628\n",
      "Epoch 46/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.6380 - mae: 0.3877 - mse: 0.6380 - val_loss: 0.3581 - val_mae: 0.3178 - val_mse: 0.3581\n",
      "Epoch 47/100\n",
      "325/325 [==============================] - 1s 5ms/step - loss: 0.6337 - mae: 0.3940 - mse: 0.6337 - val_loss: 0.4920 - val_mae: 0.2105 - val_mse: 0.4920\n",
      "Epoch 48/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.5782 - mae: 0.3383 - mse: 0.5782 - val_loss: 0.4708 - val_mae: 0.3110 - val_mse: 0.4708\n",
      "Epoch 49/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.7285 - mae: 0.4044 - mse: 0.7285 - val_loss: 0.7188 - val_mae: 0.4221 - val_mse: 0.7188\n",
      "Epoch 50/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.9834 - mae: 0.4575 - mse: 0.9834 - val_loss: 0.4327 - val_mae: 0.3066 - val_mse: 0.4327\n",
      "Epoch 51/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.5760 - mae: 0.3227 - mse: 0.5760 - val_loss: 0.5526 - val_mae: 0.2478 - val_mse: 0.5526\n",
      "Epoch 52/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.6064 - mae: 0.3345 - mse: 0.6064 - val_loss: 0.4444 - val_mae: 0.4078 - val_mse: 0.4444\n",
      "Epoch 53/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.7749 - mae: 0.4334 - mse: 0.7749 - val_loss: 0.4894 - val_mae: 0.2945 - val_mse: 0.4894\n",
      "Epoch 54/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.6124 - mae: 0.3623 - mse: 0.6124 - val_loss: 0.2746 - val_mae: 0.2525 - val_mse: 0.2746\n",
      "Epoch 55/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.6129 - mae: 0.3687 - mse: 0.6129 - val_loss: 0.7280 - val_mae: 0.5125 - val_mse: 0.7280\n",
      "Epoch 56/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.6506 - mae: 0.3558 - mse: 0.6506 - val_loss: 0.3360 - val_mae: 0.2585 - val_mse: 0.3360\n",
      "Epoch 57/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.7872 - mae: 0.4211 - mse: 0.7872 - val_loss: 0.5138 - val_mae: 0.3520 - val_mse: 0.5138\n",
      "Epoch 58/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.8308 - mae: 0.3967 - mse: 0.8308 - val_loss: 0.7618 - val_mae: 0.5087 - val_mse: 0.7618\n",
      "Epoch 59/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.6893 - mae: 0.3349 - mse: 0.6893 - val_loss: 0.4223 - val_mae: 0.2492 - val_mse: 0.4223\n",
      "Epoch 60/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.3672 - mae: 0.2507 - mse: 0.3672 - val_loss: 0.3212 - val_mae: 0.3422 - val_mse: 0.3212\n",
      "Epoch 61/100\n",
      "325/325 [==============================] - 2s 6ms/step - loss: 0.5176 - mae: 0.3279 - mse: 0.5176 - val_loss: 0.4335 - val_mae: 0.3392 - val_mse: 0.4335\n",
      "Epoch 62/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.6099 - mae: 0.3449 - mse: 0.6099 - val_loss: 0.4336 - val_mae: 0.3137 - val_mse: 0.4336\n",
      "Epoch 63/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.5202 - mae: 0.2992 - mse: 0.5202 - val_loss: 0.2688 - val_mae: 0.2335 - val_mse: 0.2688\n",
      "Epoch 64/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.4432 - mae: 0.3001 - mse: 0.4432 - val_loss: 0.3811 - val_mae: 0.3145 - val_mse: 0.3811\n",
      "Epoch 65/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.6124 - mae: 0.3711 - mse: 0.6124 - val_loss: 0.3500 - val_mae: 0.3227 - val_mse: 0.3500\n",
      "Epoch 66/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.4344 - mae: 0.2834 - mse: 0.4344 - val_loss: 0.1879 - val_mae: 0.2307 - val_mse: 0.1879\n",
      "Epoch 67/100\n",
      "325/325 [==============================] - 1s 5ms/step - loss: 0.7621 - mae: 0.3813 - mse: 0.7621 - val_loss: 1.0636 - val_mae: 0.7476 - val_mse: 1.0636\n",
      "Epoch 68/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.7988 - mae: 0.4369 - mse: 0.7988 - val_loss: 0.3495 - val_mae: 0.3285 - val_mse: 0.3495\n",
      "Epoch 69/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.4190 - mae: 0.2955 - mse: 0.4190 - val_loss: 0.2050 - val_mae: 0.1718 - val_mse: 0.2050\n",
      "Epoch 70/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.4527 - mae: 0.2883 - mse: 0.4527 - val_loss: 0.2657 - val_mae: 0.1760 - val_mse: 0.2657\n",
      "Epoch 71/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.3856 - mae: 0.2799 - mse: 0.3856 - val_loss: 0.4683 - val_mae: 0.3409 - val_mse: 0.4683\n",
      "Epoch 72/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.3777 - mae: 0.2685 - mse: 0.3777 - val_loss: 0.1904 - val_mae: 0.2597 - val_mse: 0.1904\n",
      "Epoch 73/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.2696 - mae: 0.2353 - mse: 0.2696 - val_loss: 0.3005 - val_mae: 0.2159 - val_mse: 0.3005\n",
      "Epoch 74/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.4441 - mae: 0.3155 - mse: 0.4441 - val_loss: 0.3624 - val_mae: 0.4192 - val_mse: 0.3624\n",
      "Epoch 75/100\n",
      "325/325 [==============================] - 2s 5ms/step - loss: 0.7436 - mae: 0.4169 - mse: 0.7436 - val_loss: 0.6491 - val_mae: 0.5549 - val_mse: 0.6491\n",
      "Epoch 76/100\n",
      "325/325 [==============================] - 1s 4ms/step - loss: 0.8795 - mae: 0.4069 - mse: 0.8795 - val_loss: 0.3688 - val_mae: 0.3152 - val_mse: 0.3688\n"
     ]
    }
   ],
   "source": [
    "history = model_NN.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size = 10,\n",
    "    epochs = 100,\n",
    "    validation_split = 0.2,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>mae</th>\n",
       "      <th>mse</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_mae</th>\n",
       "      <th>val_mse</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.570620</td>\n",
       "      <td>1.986422</td>\n",
       "      <td>15.570620</td>\n",
       "      <td>2.958892</td>\n",
       "      <td>1.005544</td>\n",
       "      <td>2.958892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.131809</td>\n",
       "      <td>1.109486</td>\n",
       "      <td>4.131809</td>\n",
       "      <td>2.488183</td>\n",
       "      <td>0.878960</td>\n",
       "      <td>2.488183</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.259639</td>\n",
       "      <td>0.956338</td>\n",
       "      <td>3.259639</td>\n",
       "      <td>1.948912</td>\n",
       "      <td>0.867388</td>\n",
       "      <td>1.948912</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.809633</td>\n",
       "      <td>0.968374</td>\n",
       "      <td>2.809633</td>\n",
       "      <td>2.551531</td>\n",
       "      <td>1.099493</td>\n",
       "      <td>2.551531</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.770217</td>\n",
       "      <td>0.924246</td>\n",
       "      <td>2.770217</td>\n",
       "      <td>2.095030</td>\n",
       "      <td>0.859806</td>\n",
       "      <td>2.095030</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.226658</td>\n",
       "      <td>0.749550</td>\n",
       "      <td>2.226658</td>\n",
       "      <td>1.354739</td>\n",
       "      <td>0.615134</td>\n",
       "      <td>1.354739</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.087020</td>\n",
       "      <td>0.761734</td>\n",
       "      <td>2.087020</td>\n",
       "      <td>1.600933</td>\n",
       "      <td>0.681325</td>\n",
       "      <td>1.600933</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.960893</td>\n",
       "      <td>0.721798</td>\n",
       "      <td>1.960893</td>\n",
       "      <td>1.360997</td>\n",
       "      <td>0.523235</td>\n",
       "      <td>1.360997</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.928843</td>\n",
       "      <td>0.764276</td>\n",
       "      <td>1.928843</td>\n",
       "      <td>1.858701</td>\n",
       "      <td>0.697751</td>\n",
       "      <td>1.858701</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.943350</td>\n",
       "      <td>0.727586</td>\n",
       "      <td>1.943351</td>\n",
       "      <td>2.317002</td>\n",
       "      <td>0.927037</td>\n",
       "      <td>2.317002</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.755728</td>\n",
       "      <td>0.686141</td>\n",
       "      <td>1.755728</td>\n",
       "      <td>1.206362</td>\n",
       "      <td>0.451504</td>\n",
       "      <td>1.206362</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.529343</td>\n",
       "      <td>0.611036</td>\n",
       "      <td>1.529343</td>\n",
       "      <td>1.274273</td>\n",
       "      <td>0.492487</td>\n",
       "      <td>1.274273</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.658594</td>\n",
       "      <td>0.662624</td>\n",
       "      <td>1.658594</td>\n",
       "      <td>1.289918</td>\n",
       "      <td>0.580130</td>\n",
       "      <td>1.289918</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.976557</td>\n",
       "      <td>0.702417</td>\n",
       "      <td>1.976557</td>\n",
       "      <td>1.450945</td>\n",
       "      <td>0.572791</td>\n",
       "      <td>1.450945</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.533596</td>\n",
       "      <td>0.651461</td>\n",
       "      <td>1.533596</td>\n",
       "      <td>2.038908</td>\n",
       "      <td>0.816269</td>\n",
       "      <td>2.038908</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.286348</td>\n",
       "      <td>0.568500</td>\n",
       "      <td>1.286348</td>\n",
       "      <td>2.371175</td>\n",
       "      <td>1.066938</td>\n",
       "      <td>2.371175</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.785896</td>\n",
       "      <td>0.730365</td>\n",
       "      <td>1.785896</td>\n",
       "      <td>1.272423</td>\n",
       "      <td>0.526973</td>\n",
       "      <td>1.272423</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.344200</td>\n",
       "      <td>0.555467</td>\n",
       "      <td>1.344200</td>\n",
       "      <td>0.902289</td>\n",
       "      <td>0.440664</td>\n",
       "      <td>0.902289</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.055829</td>\n",
       "      <td>0.446312</td>\n",
       "      <td>1.055829</td>\n",
       "      <td>1.236584</td>\n",
       "      <td>0.649091</td>\n",
       "      <td>1.236584</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.704442</td>\n",
       "      <td>0.691470</td>\n",
       "      <td>1.704442</td>\n",
       "      <td>1.425932</td>\n",
       "      <td>0.631049</td>\n",
       "      <td>1.425932</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.294552</td>\n",
       "      <td>0.579906</td>\n",
       "      <td>1.294552</td>\n",
       "      <td>0.613895</td>\n",
       "      <td>0.306380</td>\n",
       "      <td>0.613895</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.084476</td>\n",
       "      <td>0.492921</td>\n",
       "      <td>1.084476</td>\n",
       "      <td>1.007936</td>\n",
       "      <td>0.400272</td>\n",
       "      <td>1.007936</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.122337</td>\n",
       "      <td>0.514950</td>\n",
       "      <td>1.122337</td>\n",
       "      <td>0.931022</td>\n",
       "      <td>0.446011</td>\n",
       "      <td>0.931022</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.257272</td>\n",
       "      <td>0.562795</td>\n",
       "      <td>1.257272</td>\n",
       "      <td>0.887311</td>\n",
       "      <td>0.513228</td>\n",
       "      <td>0.887311</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.967618</td>\n",
       "      <td>0.486342</td>\n",
       "      <td>0.967618</td>\n",
       "      <td>0.616085</td>\n",
       "      <td>0.412536</td>\n",
       "      <td>0.616085</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.287454</td>\n",
       "      <td>0.599517</td>\n",
       "      <td>1.287454</td>\n",
       "      <td>0.586508</td>\n",
       "      <td>0.327461</td>\n",
       "      <td>0.586508</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.978394</td>\n",
       "      <td>0.463660</td>\n",
       "      <td>0.978394</td>\n",
       "      <td>1.982200</td>\n",
       "      <td>0.912602</td>\n",
       "      <td>1.982200</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.330660</td>\n",
       "      <td>0.588735</td>\n",
       "      <td>1.330660</td>\n",
       "      <td>0.563416</td>\n",
       "      <td>0.337087</td>\n",
       "      <td>0.563416</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.923766</td>\n",
       "      <td>0.458461</td>\n",
       "      <td>0.923766</td>\n",
       "      <td>0.823729</td>\n",
       "      <td>0.589743</td>\n",
       "      <td>0.823729</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.813897</td>\n",
       "      <td>0.450424</td>\n",
       "      <td>0.813897</td>\n",
       "      <td>0.844873</td>\n",
       "      <td>0.380862</td>\n",
       "      <td>0.844873</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1.130579</td>\n",
       "      <td>0.489105</td>\n",
       "      <td>1.130579</td>\n",
       "      <td>0.402782</td>\n",
       "      <td>0.316838</td>\n",
       "      <td>0.402782</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.941399</td>\n",
       "      <td>0.424986</td>\n",
       "      <td>0.941399</td>\n",
       "      <td>0.696538</td>\n",
       "      <td>0.371775</td>\n",
       "      <td>0.696538</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.697947</td>\n",
       "      <td>0.378783</td>\n",
       "      <td>0.697947</td>\n",
       "      <td>0.515809</td>\n",
       "      <td>0.385402</td>\n",
       "      <td>0.515809</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.802248</td>\n",
       "      <td>0.415569</td>\n",
       "      <td>0.802248</td>\n",
       "      <td>0.686835</td>\n",
       "      <td>0.388431</td>\n",
       "      <td>0.686835</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.866648</td>\n",
       "      <td>0.483631</td>\n",
       "      <td>0.866648</td>\n",
       "      <td>0.665820</td>\n",
       "      <td>0.505053</td>\n",
       "      <td>0.665820</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.855648</td>\n",
       "      <td>0.407118</td>\n",
       "      <td>0.855648</td>\n",
       "      <td>0.603501</td>\n",
       "      <td>0.412557</td>\n",
       "      <td>0.603501</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.931637</td>\n",
       "      <td>0.476405</td>\n",
       "      <td>0.931637</td>\n",
       "      <td>0.713245</td>\n",
       "      <td>0.553787</td>\n",
       "      <td>0.713245</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1.006309</td>\n",
       "      <td>0.543203</td>\n",
       "      <td>1.006309</td>\n",
       "      <td>0.694123</td>\n",
       "      <td>0.503746</td>\n",
       "      <td>0.694123</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.733415</td>\n",
       "      <td>0.401525</td>\n",
       "      <td>0.733415</td>\n",
       "      <td>0.555342</td>\n",
       "      <td>0.388044</td>\n",
       "      <td>0.555342</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.571867</td>\n",
       "      <td>0.327347</td>\n",
       "      <td>0.571867</td>\n",
       "      <td>0.386310</td>\n",
       "      <td>0.307468</td>\n",
       "      <td>0.386310</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.705825</td>\n",
       "      <td>0.390232</td>\n",
       "      <td>0.705825</td>\n",
       "      <td>0.578173</td>\n",
       "      <td>0.423116</td>\n",
       "      <td>0.578173</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.649668</td>\n",
       "      <td>0.397667</td>\n",
       "      <td>0.649668</td>\n",
       "      <td>0.422814</td>\n",
       "      <td>0.298235</td>\n",
       "      <td>0.422814</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.673625</td>\n",
       "      <td>0.388512</td>\n",
       "      <td>0.673625</td>\n",
       "      <td>0.478335</td>\n",
       "      <td>0.320598</td>\n",
       "      <td>0.478335</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1.363924</td>\n",
       "      <td>0.556987</td>\n",
       "      <td>1.363925</td>\n",
       "      <td>0.620858</td>\n",
       "      <td>0.341272</td>\n",
       "      <td>0.620858</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1.241693</td>\n",
       "      <td>0.503193</td>\n",
       "      <td>1.241693</td>\n",
       "      <td>0.662427</td>\n",
       "      <td>0.386897</td>\n",
       "      <td>0.662427</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.665251</td>\n",
       "      <td>0.353627</td>\n",
       "      <td>0.665251</td>\n",
       "      <td>0.617782</td>\n",
       "      <td>0.310944</td>\n",
       "      <td>0.617782</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.596162</td>\n",
       "      <td>0.297461</td>\n",
       "      <td>0.596162</td>\n",
       "      <td>0.820199</td>\n",
       "      <td>0.431974</td>\n",
       "      <td>0.820199</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.765279</td>\n",
       "      <td>0.402662</td>\n",
       "      <td>0.765279</td>\n",
       "      <td>0.306523</td>\n",
       "      <td>0.284575</td>\n",
       "      <td>0.306523</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.683055</td>\n",
       "      <td>0.367287</td>\n",
       "      <td>0.683055</td>\n",
       "      <td>0.566002</td>\n",
       "      <td>0.280370</td>\n",
       "      <td>0.566002</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.598820</td>\n",
       "      <td>0.332304</td>\n",
       "      <td>0.598820</td>\n",
       "      <td>0.419018</td>\n",
       "      <td>0.351868</td>\n",
       "      <td>0.419018</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.629246</td>\n",
       "      <td>0.348543</td>\n",
       "      <td>0.629246</td>\n",
       "      <td>0.401719</td>\n",
       "      <td>0.298681</td>\n",
       "      <td>0.401719</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.547874</td>\n",
       "      <td>0.324553</td>\n",
       "      <td>0.547874</td>\n",
       "      <td>0.353565</td>\n",
       "      <td>0.292619</td>\n",
       "      <td>0.353565</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.609263</td>\n",
       "      <td>0.364163</td>\n",
       "      <td>0.609263</td>\n",
       "      <td>0.447208</td>\n",
       "      <td>0.296155</td>\n",
       "      <td>0.447208</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.851157</td>\n",
       "      <td>0.429701</td>\n",
       "      <td>0.851157</td>\n",
       "      <td>1.247332</td>\n",
       "      <td>0.528132</td>\n",
       "      <td>1.247332</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.783166</td>\n",
       "      <td>0.418426</td>\n",
       "      <td>0.783166</td>\n",
       "      <td>0.595994</td>\n",
       "      <td>0.284571</td>\n",
       "      <td>0.595994</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.509454</td>\n",
       "      <td>0.303010</td>\n",
       "      <td>0.509454</td>\n",
       "      <td>0.476655</td>\n",
       "      <td>0.230157</td>\n",
       "      <td>0.476655</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.853505</td>\n",
       "      <td>0.387836</td>\n",
       "      <td>0.853505</td>\n",
       "      <td>0.929946</td>\n",
       "      <td>0.591697</td>\n",
       "      <td>0.929946</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.642164</td>\n",
       "      <td>0.359616</td>\n",
       "      <td>0.642164</td>\n",
       "      <td>0.398206</td>\n",
       "      <td>0.219589</td>\n",
       "      <td>0.398206</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         loss       mae        mse  val_loss   val_mae   val_mse  epoch\n",
       "0   15.570620  1.986422  15.570620  2.958892  1.005544  2.958892      0\n",
       "1    4.131809  1.109486   4.131809  2.488183  0.878960  2.488183      1\n",
       "2    3.259639  0.956338   3.259639  1.948912  0.867388  1.948912      2\n",
       "3    2.809633  0.968374   2.809633  2.551531  1.099493  2.551531      3\n",
       "4    2.770217  0.924246   2.770217  2.095030  0.859806  2.095030      4\n",
       "5    2.226658  0.749550   2.226658  1.354739  0.615134  1.354739      5\n",
       "6    2.087020  0.761734   2.087020  1.600933  0.681325  1.600933      6\n",
       "7    1.960893  0.721798   1.960893  1.360997  0.523235  1.360997      7\n",
       "8    1.928843  0.764276   1.928843  1.858701  0.697751  1.858701      8\n",
       "9    1.943350  0.727586   1.943351  2.317002  0.927037  2.317002      9\n",
       "10   1.755728  0.686141   1.755728  1.206362  0.451504  1.206362     10\n",
       "11   1.529343  0.611036   1.529343  1.274273  0.492487  1.274273     11\n",
       "12   1.658594  0.662624   1.658594  1.289918  0.580130  1.289918     12\n",
       "13   1.976557  0.702417   1.976557  1.450945  0.572791  1.450945     13\n",
       "14   1.533596  0.651461   1.533596  2.038908  0.816269  2.038908     14\n",
       "15   1.286348  0.568500   1.286348  2.371175  1.066938  2.371175     15\n",
       "16   1.785896  0.730365   1.785896  1.272423  0.526973  1.272423     16\n",
       "17   1.344200  0.555467   1.344200  0.902289  0.440664  0.902289     17\n",
       "18   1.055829  0.446312   1.055829  1.236584  0.649091  1.236584     18\n",
       "19   1.704442  0.691470   1.704442  1.425932  0.631049  1.425932     19\n",
       "20   1.294552  0.579906   1.294552  0.613895  0.306380  0.613895     20\n",
       "21   1.084476  0.492921   1.084476  1.007936  0.400272  1.007936     21\n",
       "22   1.122337  0.514950   1.122337  0.931022  0.446011  0.931022     22\n",
       "23   1.257272  0.562795   1.257272  0.887311  0.513228  0.887311     23\n",
       "24   0.967618  0.486342   0.967618  0.616085  0.412536  0.616085     24\n",
       "25   1.287454  0.599517   1.287454  0.586508  0.327461  0.586508     25\n",
       "26   0.978394  0.463660   0.978394  1.982200  0.912602  1.982200     26\n",
       "27   1.330660  0.588735   1.330660  0.563416  0.337087  0.563416     27\n",
       "28   0.923766  0.458461   0.923766  0.823729  0.589743  0.823729     28\n",
       "29   0.813897  0.450424   0.813897  0.844873  0.380862  0.844873     29\n",
       "30   1.130579  0.489105   1.130579  0.402782  0.316838  0.402782     30\n",
       "31   0.941399  0.424986   0.941399  0.696538  0.371775  0.696538     31\n",
       "32   0.697947  0.378783   0.697947  0.515809  0.385402  0.515809     32\n",
       "33   0.802248  0.415569   0.802248  0.686835  0.388431  0.686835     33\n",
       "34   0.866648  0.483631   0.866648  0.665820  0.505053  0.665820     34\n",
       "35   0.855648  0.407118   0.855648  0.603501  0.412557  0.603501     35\n",
       "36   0.931637  0.476405   0.931637  0.713245  0.553787  0.713245     36\n",
       "37   1.006309  0.543203   1.006309  0.694123  0.503746  0.694123     37\n",
       "38   0.733415  0.401525   0.733415  0.555342  0.388044  0.555342     38\n",
       "39   0.571867  0.327347   0.571867  0.386310  0.307468  0.386310     39\n",
       "40   0.705825  0.390232   0.705825  0.578173  0.423116  0.578173     40\n",
       "41   0.649668  0.397667   0.649668  0.422814  0.298235  0.422814     41\n",
       "42   0.673625  0.388512   0.673625  0.478335  0.320598  0.478335     42\n",
       "43   1.363924  0.556987   1.363925  0.620858  0.341272  0.620858     43\n",
       "44   1.241693  0.503193   1.241693  0.662427  0.386897  0.662427     44\n",
       "45   0.665251  0.353627   0.665251  0.617782  0.310944  0.617782     45\n",
       "46   0.596162  0.297461   0.596162  0.820199  0.431974  0.820199     46\n",
       "47   0.765279  0.402662   0.765279  0.306523  0.284575  0.306523     47\n",
       "48   0.683055  0.367287   0.683055  0.566002  0.280370  0.566002     48\n",
       "49   0.598820  0.332304   0.598820  0.419018  0.351868  0.419018     49\n",
       "50   0.629246  0.348543   0.629246  0.401719  0.298681  0.401719     50\n",
       "51   0.547874  0.324553   0.547874  0.353565  0.292619  0.353565     51\n",
       "52   0.609263  0.364163   0.609263  0.447208  0.296155  0.447208     52\n",
       "53   0.851157  0.429701   0.851157  1.247332  0.528132  1.247332     53\n",
       "54   0.783166  0.418426   0.783166  0.595994  0.284571  0.595994     54\n",
       "55   0.509454  0.303010   0.509454  0.476655  0.230157  0.476655     55\n",
       "56   0.853505  0.387836   0.853505  0.929946  0.591697  0.929946     56\n",
       "57   0.642164  0.359616   0.642164  0.398206  0.219589  0.398206     57"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hist = pd.DataFrame(history.history)\n",
    "df_hist['epoch'] = history.epoch\n",
    "df_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21958930790424347"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hist['val_mae'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    df_hist = pd.DataFrame(history.history)\n",
    "    df_hist['epoch'] = history.epoch\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.plot(df_hist['epoch'], df_hist['mae'], label=['Train Error'] )\n",
    "    plt.plot(df_hist['epoch'], df_hist['val_mae'], label=['Val Error'] )\n",
    "    plt.legend()\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.plot(df_hist['epoch'], df_hist['mse'], label=['Train Error'] )\n",
    "    plt.plot(df_hist['epoch'], df_hist['val_mse'], label=['Val Error'] )\n",
    "    plt.ylim([0,20])\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparation of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grid</th>\n",
       "      <th>Best score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Extra Tree</td>\n",
       "      <td>-0.009849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>-0.038432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighbors</td>\n",
       "      <td>-0.039871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>-0.047538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CatBoost</td>\n",
       "      <td>-0.072469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>XGB wrapped</td>\n",
       "      <td>-0.080796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LGBM wrapped</td>\n",
       "      <td>-0.094547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NN</td>\n",
       "      <td>-0.219589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gradient Boosting wrapped</td>\n",
       "      <td>-1.328357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>-5.731186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>-5.741624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Grid  Best score\n",
       "3                  Extra Tree   -0.009849\n",
       "4               Decision Tree   -0.038432\n",
       "1                  KNeighbors   -0.039871\n",
       "5               Random Forest   -0.047538\n",
       "10                   CatBoost   -0.072469\n",
       "6                 XGB wrapped   -0.080796\n",
       "9                LGBM wrapped   -0.094547\n",
       "11                         NN   -0.219589\n",
       "2   Gradient Boosting wrapped   -1.328357\n",
       "7                       Ridge   -5.731186\n",
       "0           Linear Regression   -5.741624"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_models = best_grids.copy()\n",
    "best_models.loc[len(best_models.index)] = ['NN', -df_hist['val_mae'].min()]\n",
    "best_models = best_models.sort_values(by=\"Best score\", ascending=False)\n",
    "best_models.head(len(best_grids.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models.to_excel(ROOT_PATH + '\\\\results\\\\models_score.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "extratree = Pipeline([\n",
    "                (\"extratree\",ExtraTreesRegressor())\n",
    "               ])\n",
    "\n",
    "neighbors = Pipeline([\n",
    "                (\"neighbors\",KNeighborsRegressor())\n",
    "               ])\n",
    "\n",
    "\n",
    "decisionTree = Pipeline([\n",
    "                    (\"decisionTree\",DecisionTreeRegressor())\n",
    "                   ])\n",
    "\n",
    "# Extra Tree\n",
    "grid_extraTree = {\"extratree__max_depth\":list(range(1,10)) # Profundidades del árbol. Cuanto más profundo, mas posibilidades de overfitting,\n",
    "                                            # pero  mas preciso en entrenamiento.\n",
    "              }\n",
    "\n",
    "# KNN\n",
    "grid_neighbors = {\"neighbors__n_neighbors\": [3,5,7,9,11],       # Pares acepta sklearn, pero se suele poner impares, por los empates\n",
    "                  \"neighbors__weights\": [\"uniform\",\"distance\"]  # Ponderar o no las clasificaciones en \n",
    "                                                     # función de la inversa de la distancia a cada vecino\n",
    "                  }\n",
    "\n",
    "# ARBOL DE DECISION\n",
    "grid_decisionTree = {\"decisionTree__max_depth\":list(range(1,10)) # Profundidades del árbol. Cuanto más profundo, mas posibilidades de overfitting,\n",
    "                                            # pero  mas preciso en entrenamiento.\n",
    "              }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    }
   ],
   "source": [
    "# Almaceno en una lista de tuplas los modelos (nombre que le pongo, el modelo, hiperparametros)\n",
    "models = [('Extra Trees Regressor', extratree, grid_extraTree),\n",
    "            ('KNeighbors Regressor', neighbors, grid_neighbors),\n",
    "            ('Decision Tree Regressor', decisionTree, grid_decisionTree),\n",
    "         ]\n",
    "\n",
    "# Declaro en un diccionario los pipelines e hiperparametros\n",
    "models_gridsearch = {}\n",
    "\n",
    "for m in models:\n",
    "    models_gridsearch[m[0]] = GridSearchCV(m[1],\n",
    "                                          m[2],\n",
    "                                          cv=5,\n",
    "                                          scoring=\"neg_mean_squared_error\",\n",
    "                                          verbose=1,\n",
    "                                          n_jobs=-1)\n",
    "    \n",
    "    models_gridsearch[m[0]].fit(X_train, y_train)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Grid</th>\n",
       "      <th>Best score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNeighbors Regressor</td>\n",
       "      <td>-0.001288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Extra Trees Regressor</td>\n",
       "      <td>-0.161189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree Regressor</td>\n",
       "      <td>-0.255292</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Grid  Best score\n",
       "1     KNeighbors Regressor   -0.001288\n",
       "0    Extra Trees Regressor   -0.161189\n",
       "2  Decision Tree Regressor   -0.255292"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_grids = [(i, j.best_score_) for i, j in models_gridsearch.items()]\n",
    "\n",
    "best_grids = pd.DataFrame(best_grids, columns=[\"Grid\", \"Best score\"]).sort_values(by=\"Best score\", ascending=False)\n",
    "best_grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('neighbors',\n",
       "                 KNeighborsRegressor(n_neighbors=3, weights='distance'))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# El mejor ha sido KNN\n",
    "models_gridsearch['KNeighbors Regressor'].best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20.4  4.8]\n",
      " [15.6  6. ]\n",
      " [31.2 29.4]\n",
      " ...\n",
      " [ 6.6  6. ]\n",
      " [27.6  6. ]\n",
      " [36.  17.4]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\src\\theBridge\\course-env\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = KNeighborsRegressor(n_neighbors=3, weights='distance')\n",
    "# model = DecisionTreeRegressor()\n",
    "model.fit(X_train, y_train)\n",
    "print(model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real point          x    y\n",
      "6520  19.8  7.8\n",
      "Point predicted [[19.8  7.8]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\src\\theBridge\\course-env\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "i = 150\n",
    "# test = X_test.iloc[i:i+1, :]\n",
    "print(f'Real point {y_test.iloc[i:i+1, :]}')\n",
    "print(f'Point predicted {model.predict(X_test.iloc[i:i+1:, :])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.0000004 \n",
      "RMSE: 0.00064 \n",
      "MAE:  0.00002 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\src\\theBridge\\course-env\\lib\\site-packages\\sklearn\\base.py:442: UserWarning: X does not have valid feature names, but KNeighborsRegressor was fitted with feature names\n",
      "  \"X does not have valid feature names, but\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print(\"MSE:  %.7f \" % mean_squared_error(y_test, predictions))\n",
    "print(\"RMSE: %.5f \" % np.sqrt(mean_squared_error(y_test, predictions)))\n",
    "print(\"MAE:  %.5f \" % mean_absolute_error(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('KNeighborsRegressor_nb.model', \"wb\") as archivo_salida:\n",
    "    pickle.dump(model, archivo_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsRegressor(n_neighbors=3, weights='distance')\n"
     ]
    }
   ],
   "source": [
    "# Para volver a leer el modelo\n",
    "with open('KNeighborsRegressor_nb.model', \"rb\") as archivo_entrada:\n",
    "    import_model = pickle.load(archivo_entrada)\n",
    "    \n",
    "print(import_model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9fe7e7741b3f19bcd135589f0a4246388c4ceff90e79f616981a3065da97231"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('course-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
